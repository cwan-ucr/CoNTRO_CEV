{
    "episode":{
        "num_train_rollouts": 300,
        "rollout_length": 2048,
        "eval_freq": 50,
        "eval_num_eps": 25,
        "max_ep_steps": 1250,
        "generation_ep_steps": 1000,
        "warmup_ep_steps": 0,
        "test_num_eps": 25
    },
    "agent": {
        "agent_type": "ppo",
        "single_agent": false,
        "centralized_train": false,
        "independent_train": true
    },
    "ppo":{
        "gae_tau": 0.85,
        "entropy_weight": 0.2,
        "minibatch_size": 256,
        "optimization_epochs": 5,
        "ppo_ratio_clip": 0.3,
        "discount": 0.99,
        "learning_rate": 1e-4,
        "clip_grads": true,
        "gradient_clip": 1.0,
        "value_loss_coef": 1.0,
        "hidden_layer_size": [32, 32],
        "actor_layer_size": [32, 32],
        "critic_layer_size": [32, 32]
    },
    "rule": {
        "rule_set": "cycle",
        "rule_set_params": {"cycle_length": 120, "NS_mult": 1.0, "EW_mult": 1.0, "phase_end_offset": 100}
    },
    "environment": {
        "shape": [4, 4],
        "rush_hour": false,
        "uniform_generation_probability": 0.06,
        "use_vehicle_controller": true,
        "step_length": 1,
        "gather_vehicle_info": false,
        "actuated_tls": false,
        "phase_mode": "hybrid",
        "vehicle_type_distribution": [0.4, 0.2, 0.2, 0.2]
    },
    "multiagent": {
        "reward_interpolation": 0,
        "state_interpolation": 0
    },
    "parallel":{
        "num_workers": 1
    }
}